\section{Frameworks and Tools}

\subsection{Extraction}

\subsubsection{Common Crawl}
Common Crawl \cite{commoncrawl} is a freely accessible corpus of the pages across the web. Their data is updated and released on a monthly basis. Many researchers have used the data for varying purposes~\cite{smith2013dirt}~\cite{muhleisen2012web}~\cite{singh2012wikilinks}. Since the UrbanSearch project requires us to crawl the web (see section {\color{Red} FIXME}), the corpus is a very suitable candidate for us to work with.

The data of Common Crawl comes in three formats\footnote{\url{https://gist.github.com/Smerity/e750f0ef0ab9aa366558}}: 
\begin{itemize}
\item[\textbf{WARC}] This is the default and most verbose format. It stores the HTTP-response, information about the request and meta-data on the crawl process itself. The content is stored as HTML-content.
\item[\textbf{WAT}] Files of this type contain important meta-data, such as link addresses, about the WARC-records. This meta-data is computed for each of the three types of records (meta-data, request, and response). The textual content of the page is not present in this format.
\item[\textbf{WET}] This format only contains extracted plain text. No HTML tags are present in this text. For our purposes, this is the most useful format.
\end{itemize}

For extracting data from Common Crawl, many open-source libraries are available. Common Crawls' official website refers to \texttt{cdx-index-client}\footnote{\url{https://github.com/ikreymer/cdx-index-client}} as a command line interface to their data. It allows for, among others, specifying which index to use, supports multiple output formats (plain text, gzip or JSON) and can run in parallel.

To be able to use the data on our own server, an estimation must be made on the required resources. A simple query on the latest index using the online interface\footnote{\url{http://index.commoncrawl.org/CC-MAIN-2017-13-index?url=*.nl&output=json&showNumPages=true}} yields 1676 pages of 15000 entries each, which are roughly 25 million entries in total. However, there are over 5.5 million registered domain names with top level domain \texttt{.nl}\footnote{\url{https://www.sidn.nl/a/knowledge-and-development/statistics?language_id=2}}. One would expect many more pages to exist with that number of domains. There are several explanations for this, including:
\begin{itemize}
\item Common large websites, such as \url{www.google.nl} and \url{www.wikipedia.nl} have not been fully indexed by Common Crawl, because their "parents", \url{www.google.com} and \url{www.wikipedia.org} have already been indexed almost entirely.
\item Not every website allows their pages to be crawled. According to Common Crawls' official website, their bots can be blocked via the common \texttt{robots.txt}. Additionally, they honor so-called \texttt{no-follow} attributes that prevents the crawler to follow embedded links. Sites that use these features are therefore partially or not at all included in the indices of Common Crawl.
\end{itemize}

\subsubsection{Eurostat}
Eurostat is the statistical office of the European Union situated in Luxembourg. Its mission is to provide high quality statistics for Europe \cite{Eurostat}. We identified Eurostat as a source that is not useful for this particular problem. Although Eurostat contains a lot of statistics on European cities, there is not enough useful information which contributes to giving more insight into the network connectivity of cities. Therefore, we did not include Eurostat as an information source.

\subsection{Filtering and Categorizing}

\subsubsection{Clustering}
\subsubsection{Filtering}
\subsubsection{Machine Learning}
Machine learning for website classification works by using the raw text from a website. Because the text from websites is often unstructured the 'bag of words' model is used. This model counts how often each word is used. Afterwards the machine learning works in 4 steps:
\begin{enumerate}
    \item \textbf{Creating a feature extractor} \\
    Given text from a website, returns the "features" from this text. Features are the words that occur in the text and the number of occurences. Before this data is extracted stop words (the, is, at etc) are removed and the rest of the words are stemmed meaning all words will be changed to their root-forms (features - feature, controlled - control).
    \item \textbf{Manually labelling} \\
    The first set of examples (which for each class can be just a few of websites), will be manually labeled. 
    \item \todo{TODO} \textbf{Generating a classifier} \\
    The labelled examples are fed to a learning algorithms. This will generate a classifier. Several algorithms are:
    \begin{enumerate}
        \item SVM - LibSVM $(O(n^3)?) / Liblinear$
        \item Naive Bayes
        \item Neural Networks
        \item Descision trees (usually C4.5)
        \item scikit-learn
    \end{enumerate}
    \item \textbf{Entering new examples} \\
    When a new (unlabeled) example (website) comes - extract the features and feed it to your classifier - it will tell you what it thinks it is (and usually - what is the probability the classifier is correct). Afterwards the classifier is updated to include new features extracted from the example.
\end{enumerate}

\todo{TODO} There are 2 programs should be looked at: \\
Weka: feature extractor and classifier (http://www.cs.waikato.ac.nz/ml/weka/) \\ 
Scikit-learn: ? (http://scikit-learn.org/stable/) %TODO

\subsubsection{TF-IDF}
basic idea: 1. using training data to assign values on words - filter meaningless words - assign words with highest value as categories? 2. Do the same on training data for each category (choose a few documents manually per category) and then check for websites for which categories has the highest value.

\subsection{Search Queries}

\subsubsection{Enter Queries}
\subsubsection{Get Results}
\subsubsection{Specifications}

\subsection{Visualisation}
\subsubsection{neo4j?}

\subsubsection{Connection between cities}
\subsubsection{The Strength of these connections}