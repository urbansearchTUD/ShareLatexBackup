\subsection{Extracting Relations from Documents}
Now that a selection of relevant documents has been made, we can make an attempt to identify the relations between cities based on these documents. Since labelling every relevant document by hand is not feasible, an automated approach is desirable. 
One way to automate this process is by identifying intercity relations using machine learning. Machine learning algorithms can be roughly divided into two distinct groups: (1) supervised and (2) unsupervised algorithms. Supervised algorithms expect an input set and a corresponding output set, with which a model is trained to predict unseen instances of the problem. Unsupervised algorithms identify clusters of entities based on similarities in the feature set corresponding to said entity.

Considering the fact that we have a strict time schedule of little over two months to develop the complete system, we decided to go with the supervised approach. 
Another reason for this choice is that the training and tweaking of supervised algorithms can be done faster compared to unsupervised algorithms. This stems from the fact that we do not need the complete data-set to start training a supervised model, while for the unsupervised case the complete set is needed.

\subsubsection{Defining Classes}
Our choice of using classification has naturally lead to the need for categories we want to identify within the collected documents. Together with our clients we identified the following categories which are useful to identify from the collected documents:\\

\begin{enumerate}
    \item Collaboration
    \item Commuting
    \item Education
    \item Leisure
    \item Residential mobility
    \item Shopping
    \item Transportation
    \item Other
\end{enumerate}

These categories represent topics that are of interest for our clients. They relate to research that is being done by them and to relations that were deemed important in previous research on intercity relations. The category \textit{other} is there to make the classification exhaustive, i.e. relevant documents can always be labelled.

\subsubsection{Pre-processing}
For pre-processing the documents, there are a number of tools available. We used NLTK \cite{nlkt_stemming} for removing stopwords and regular expressions for removing unwanted characters. The HTML parsing is done using BeautifulSoup\cite{BeautifulSoup}.

\begin{description}
\item[\textbf{Stop words}]
Removing all common words (the, a, an etc) and symbols ('.', ',', '!', etc). For removing stopwords, we used a list from NLTK containing Dutch stopwords.

\item[\textbf{Unwanted characters}]
To strip unwanted characters we have defined a regular expression that identifies unwanted characters (punctuation marks, years, etc.) Matching characters are removed from the document. 

\item[\textbf{HTML}]
Since we are dealing with HTML pages which we are parsing to plain text documents, we need to strip the HTML so that only the plain text remains. Using BeautifulSoup we strip unwanted tags (script, style, link, etc.) and parse the rest of the page to plain text.
\end{description}

\subsubsection{Data-set}
Before we can start building up and training our model, we need to collect data that can be used as input for the model. To collect this data we have considered several options.\\

The first option is to query for documents from news(paper) sites. Since the documents are categorised by professionals, we may assume they will provide us with features that represent the categories associated with these documents well.
Unfortunately, the categories that we identified with our clients do not match typical newspaper categories, so this approach was not suitable for us.\\

Another approach is to use Google Custom Search to obtain results from Google, using the categories the client provided us with as keywords. The main disadvantage of this approach is lack of control over the files that get added into the data set. This way documents that get returned by the query are not analysed on desirable content but are added immediately.\\

Finally, we decided to provide our clients with a "labelling interface". The labelling interface provides the user with a document from the set of collected documents. It allows the user to label these documents with zero or more categories, after which the document is assigned to the correct category. If no category is selected, the document is discarded from the training set. This way, we have total control of the documents that are added to the data set. The documents are labelled by experts in the field of the built environment so we may assume these documents will represent the labelled categories well. 

\subsubsection{Modelling}
When considering classification, there are a plethora of algorithms available. When choosing the right algorithm for a problem, several factors should be taken into account\cite{MLCheatSheet}. These are:
    \begin{description}
        \item[\textbf{Accuracy}] How well the algorithm separates the websites.
        \item[\textbf{Training Time}] How long it takes to train the algorithm.
        \item[\textbf{Linearity}] Linear regression assumes data trends follow a straight line. This is trade-off between accuracy and speed.
        \item[\textbf{Number of Parameters}] Adjustable parameters increase the flexibility of the algorithms. This is a trade-off between training time and accuracy.
        \item[\textbf{Number of Features}] A large number of features can make some algorithms slow. Especially text data (what we are using) has a high number of features.
        \item[\textbf{Special Cases}] Some learning algorithms make particular assumptions about the data or the results.
    \end{description}

Keeping all these properties in mind we construct a setup that fits our purposes best.
Below we have stated our approach of how we reached the setup we think is best suited for our goals.

\begin{description}
\item[\textbf{Features}] 
To get a useful set of inputs (features) for our system we need to decide what describes the properties of our documents best. Since we are dealing with text-documents a natural choice for these inputs are the words contained in these documents. 
The words alone do not provide us a very useful input to the system. That is why we use TF-IDF to give the words that we encountered a weight. TF-IDF (Term Frequency over Inverse Document Frequency) gives words a weight based on their frequency in a document and on the frequency of the word in the complete document set. This way words that are rare in the complete document set but occur often in a document are assigned a high weight. Words that occur in many documents in the complete document set get awarded a low weight\cite{ramos_tfidf}.
Using TF-IDF our features become words with weights associated to them.

\item[\textbf{Dimensionality reduction}]
Since we are working with text documents and our features are words with TF-IDF weights we can assume that our feature set will be very large. The total number of features determines how fast we can train our model and has implications regarding over-fitting \cite{ml_text}. To reduce the number of features we considered different techniques from \cite{ml_text}. Since we have no time to test all the techniques, we decided to select the top ten percent of our features (based on the TF-IDF weights). In \cite{yang1997} it is stated that a dimensionality reduction with a factor ten using this approach does not lead to a loss in accuracy. To provide an easy way to add different types of dimensionality reduction techniques later, we will keep the code for defining new classifiers easily extendable.

\item[\textbf{Classification}]
Even after applying dimensionality reduction which we discussed in the previous section, we are left with a lot of features. Thus, we need an algorithm that works well with a feature rich problem. From \cite{MLCheatSheet} we know SVM is a algorithm that works well with feature rich problems. Also \cite{ml_text} claims SVM is one of the best techniques when considering text classification. This combined with the fact that Scikit offers an easy to use implementation of SVM has lead us to use SVM as our classification algorithm.
\end{description}

\subsubsection{Remarks}
Scikit offers a lot of useful features to optimise the classifier. For example, using Scikit-"Pipelines" composing a classifier is an relatively easy task. Since we unfortunately do not have the time to benchmark the results of different types of classifiers and to play around with the different optimisation options, we plan on implementing our code in such a way that extending the code to use these optimising functionality and different pipelines will be really easy.