\subsection{Extracting Relations from Documents}
Now that a selection of relevant documents has been made, we can make an attempt to identify the relations between cities based on these documents. Since labelling every relevant document by hand is not feasible an automated approach is desirable. 
One way to automate this process is by identifying intercity relations using machine learning. Machine learning algorithms can be roughly divided into two distinct groups: Supervised and unsupervised algorithms. Supervised algorithms expect an input set and a corresponding output set by which an model is trained to predict unseen instances of the problem. Unsupervised algorithms identify clusters of entities based on similarities in the feature set corresponding to said entity.
Considering the fact that we have a strict time schedule of only two months to develop the complete system, we decided with our client to go with the supervised approach. 
The main reason for this choice is that the training and tweaking of supervised algorithms can be done faster compared to unsupervised algorithms. The main reason for this is that we do not need the complete data-set to start training a supervised model, while for the unsupervised case the complete set is needed.

\subsubsection{Defining Classes}
Our choice of using classification has naturally lead to the need for categories we want to identify within the collected documents. Together with our clients, Dr. Evert Meijers and Antoine Peris, we identified the following categories which are useful to identify from the collected documents:\\

\begin{enumerate}
    \item Commuting
    \item Leisure
    \item Residential mobility
    \item Education
    \item Collaboration
    \item Transportation
    \item Other
\end{enumerate}

These categories represent topics that are of interest for our clients. They relate to research that is being done by our clients and to relations that were deemed important in previous research on intercity relations.

\subsubsection{Pre-processing}

For pre-processing the documents there are a number of tools available. We used NLTK \cite{nlkt_stemming} for removing stopwords and regular expressions for removing unwanted characters. The HTML parsing/.. was done using BeautifulSoup.\todo{add ref}

\begin{enumerate}
\item Stop words. \\ 
Removing all common words (the, a, an etc) and symbols ('.', ',', '!', etc). For removing stopwords we used a list from NLTK containing dutch stopwords.

\item Unwanted characters. \\
To strip unwanted characters we have developed our own regular expression, which helps us identify and strip instances of unwanted characters (punctuation marks, years, etc.). 

\item HTML. \\
Since we are dealing with HTML pages which we are parsing to plain-text documents we need to strip unwanted content from the HTML. Using BeautifulSoup we strip unwanted tags (script, style, link, etc.) and parse the rest of the page to plain-text.
\end{enumerate}

\subsubsection{Data-set}

\subsubsection{Modelling}
When considering classification there are a multitude of algorithms available. When choosing the right algorithm for a problem several factors should be taken into account. These are \cite{MLCheatSheet}:
    \begin{enumerate}
        \item Accuracy - How well the algorithm separates the websites.
        \item Training Time - How long it takes to train the algorithm.
        \item Linearity - Linear regression assumes data trends follow a straight line. This is trade-off between accuracy and speed.
        \item Number of Parameters - Adjustable parameters increase the flexibility of the algorithms. This is a trade-off between training time and accuracy.
        \item Number of Features - A large number of features can make some algorithms unfeasible long. Especially text data (what we are using!) has a high number features.
        \item Special Cases - Some learning algorithms make particular assumptions about the data or the results.
    \end{enumerate}

Regarding the fact that we are dealing with textual data, we can assume that we will have a large feature set. An algorithm like SVM \cite{ml_text} works well with large feature sets\cite{MLCheatSheet}.
Also previous research concerning the classification of text-documents suggests SVM is one of the leading techniques in this regard\cite{ml_text}.

    
For textual data especially support vector machines are recommended\cite{MLCheatSheet, ml_text}, so it is most likely we will choose that machine learning algorithm. Depending on whether we have time we might do some tests before making our decision however. The Scikit package\cite{scikit-learn} provides an easy to implement module to use this.

\begin{enumerate}
\item TF-IDF \\ 
Before we can enter this tokenized data into the machine learning algorithm we first need to transform it to the correct model. Gensim provides two ready to use method for this to change tokenized text into the proper input data. The first is token2ID which gives each unique word a numeric ID and counts the amount that those words occur in a text and the second is tfidf which changes the amount that words occur in tf-idf values. tf-idf means term frequency inverse document frequency. Term frequency is the number of times the word occurs in a document and idf is the number of documents divided by the number of documents where the term occurs. These two values are then multiplied. The results are normalised to make up for different document lengths [https://radimrehurek.com/gensim/models/tfidfmodel.html].
\item ANOVA

\item SVM
\end{enumerate}

\subsubsection{training}
In order to use supervised machine learning, we first need to find training data for each class. We thought about two ways to do this. In order to get a good training sample for these categories, we discussed two different methods. \\

The first option was to query for results from news(paper) sites. However, this approach did not give us the desired results. The reason for this is that the categories we find in the newspapers do not match the categories Antoine and Evert provided us with. We also tried using the search engine from those news sites, however using those we found completely unrelated articles to the search queries (A search query about 'verhuizen' resulted in an article about parrots and minecraft).\\

The second option we are considering is to use Google Custom Search to obtain results from Google using the categories/keywords Antoine and Evert provided us with. A quick test with this last method provided us with quite good results (although we still see some noise in the results this is less than with the search engines from news sites).\\


\subsubsection{classifying}



\begin{comment}
First, one or more subjects should be extracted from each relevant document. These subjects should reflect the contents of the document well in order to adequately identify relationships.
The second sub-task is to determine the relationships between the cities mentioned in the document, based on the extracted subjects.

% \\
% The first challenge we face is to extract one or more subjects from each document that we selected in the previous step. These subjects should be a good description of what is stated in the document and will be the main source of identifying the relations between the cities cited in the document.
% \\
% The second challenge is, using the former mentioned subjects, to identify relations based on these subjects. 
% \\
% We will consider several possibilities which can help us in tackling these challenges in the sections below.


\subsubsection{Extracting Subjects from Documents}
To identify the subjects associated with each individual document, we investigated several algorithms that we will present next.

\begin{description}
    \item[RAKE] offers an unsupervised, domain-independent, and language-independent method for extracting keywords from individual documents \cite{rose2010automatic}. This fits well within the context of our application. We work with individual documents which we want to label with one or more subjects/keywords.
    \item[Gensim]
    \item[NLTK]
\end{description}


\subsubsection{Identifying Relations Based on Subjects}


\end{comment}