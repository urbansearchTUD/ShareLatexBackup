\subsection{Extracting Relations from Documents}
Now that a selection of relevant documents has been made, we can make an attempt to identify the relations between cities based on these documents. Since labelling every relevant document by hand is not feasible an automated approach is desirable. 
One way to automate this process is by identifying intercity relations using machine learning. Machine learning algorithms can be roughly divided into two distinct groups: Supervised and unsupervised algorithms. Supervised algorithms expect an input set and a corresponding output set by which an model is trained to predict unseen instances of the problem. Unsupervised algorithms identify clusters of entities based on similarities in the feature set corresponding to said entity.
Considering the fact that we have a strict time schedule of only two months to develop the complete system, we decided with our client to go with the supervised approach. 
The main reason for this choice is that the training and tweaking of supervised algorithms can be done faster compared to unsupervised algorithms. The main reason for this is that we do not need the complete data-set to start training a supervised model, while for the unsupervised case the complete set is needed.

\subsubsection{Defining Classes}
Our choice of using classification has naturally lead to the need for categories we want to identify within the collected documents. Together with our clients, Dr. Evert Meijers and Antoine Peris, we identified the following categories which are useful to identify from the collected documents:\\

\begin{enumerate}
    \item Commuting
    \item Leisure
    \item Residential mobility
    \item Education
    \item Collaboration
    \item Transportation
    \item Other
\end{enumerate}

These categories represent topics that are of interest for our clients. They relate to research that is being done by our clients and to relations that were deemed important in previous research on intercity relations.

\subsubsection{Pre-processing}

For pre-processing the documents there are a number of tools available. We used NLTK \cite{nlkt_stemming} for removing stopwords and regular expressions for removing unwanted characters. The HTML parsing/.. was done using BeautifulSoup.\todo{add ref}

\begin{enumerate}
\item Stop words. \\ 
Removing all common words (the, a, an etc) and symbols ('.', ',', '!', etc). For removing stopwords we used a list from NLTK containing dutch stopwords.

\item Unwanted characters. \\
To strip unwanted characters we have developed our own regular expression, which helps us identify and strip instances of unwanted characters (punctuation marks, years, etc.). 

\item HTML. \\
Since we are dealing with HTML pages which we are parsing to plain-text documents we need to strip unwanted content from the HTML. Using BeautifulSoup we strip unwanted tags (script, style, link, etc.) and parse the rest of the page to plain-text.
\end{enumerate}

\subsubsection{Data-set}

\subsubsection{Modelling}
When considering classification there are a multitude of algorithms available. When choosing the right algorithm for a problem several factors should be taken into account. These are \cite{MLCheatSheet}:
    \begin{enumerate}
        \item Accuracy - How well the algorithm separates the websites.
        \item Training Time - How long it takes to train the algorithm.
        \item Linearity - Linear regression assumes data trends follow a straight line. This is trade-off between accuracy and speed.
        \item Number of Parameters - Adjustable parameters increase the flexibility of the algorithms. This is a trade-off between training time and accuracy.
        \item Number of Features - A large number of features can make some algorithms unfeasible long. Especially text data (what we are using!) has a high number features.
        \item Special Cases - Some learning algorithms make particular assumptions about the data or the results.
    \end{enumerate}

Regarding the fact that we are dealing with textual data, we can assume that we will have a large feature set. An algorithm like SVM \cite{ml_text} works well with large feature sets\cite{MLCheatSheet}.
Also previous research concerning the classification of text-documents suggests SVM is one of the leading techniques in this regard\cite{ml_text}. While we are using the Scikit library for this project 


\begin{enumerate}
\item Features: TF-IDF \\ 
To get an useful set of inputs (features) for our system we need to decide what describes the properties of our documents best. Since we are dealing with text-documents a natural choice for these inputs are the words contained in these documents. 
The words alone do not provide us a very useful input to the system. That is why we use TF-IDF to give the words that we encountered a weight. TF-IDF (Term Frequency-Inverse Document Frequency) gives words a weight based on their frequency in a document and on the frequency of the word in the complete document set. This way words that are rare in the complete document set but occur often in a document are assigned a high weight. Words that occur in many documents in the complete document set get awarded a low weight\cite{ramos_tfidf}.
Using TF-IDF our features become words with weights associated to them.

\item Dimensionality reduction: 
Since we are working with text-documents and our features are words with TF-IDF weights we can assume that our feature set will be very large. The total number of features determines how fast we can train our model and has implications regarding over-fitting \cite{ml_text}. To reduce the number of features we consider different techniques to reduce these features. From \cite{ml_text} we 


\item SVM
\end{enumerate}

\subsubsection{training}
In order to use supervised machine learning, we first need to find training data for each class. We thought about two ways to do this. In order to get a good training sample for these categories, we discussed two different methods. \\

The first option was to query for results from news(paper) sites. However, this approach did not give us the desired results. The reason for this is that the categories we find in the newspapers do not match the categories Antoine and Evert provided us with. We also tried using the search engine from those news sites, however using those we found completely unrelated articles to the search queries (A search query about 'verhuizen' resulted in an article about parrots and minecraft).\\

The second option we are considering is to use Google Custom Search to obtain results from Google using the categories/keywords Antoine and Evert provided us with. A quick test with this last method provided us with quite good results (although we still see some noise in the results this is less than with the search engines from news sites).\\