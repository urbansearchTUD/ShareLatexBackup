\subsection{Extracting Relations from Documents}
Now that a selection of relevant documents has been made, we can identify the relations between cities based on these documents. Text documents can be split into two types: structured and unstructured. Text is structured when sentences are used, meaning grammar is used. In the case of structured texts a word may say something about the next word in the sentence, therefor other techniques can be used (for example n-grams) then in case of unstructured texts.Because the text from websites can be structured as well as unstructured, we will most likely use unstructured approaches. One way to do this is by using machine learning. The two common approaches are supervised classification and unsupervised clustering. \\

Clustering, is sorting these websites into groups based on how much they are alike. It does however not assign these groups with a label (or class). this can be a large problem because in one group may be several topics that have similarities with some of the other topics within the group but not with all other topics in the group, which makes it very hard to assign a label to these groups. Although it is possible to label these groups manually, since it is quit tedious and we are on a limited time schedule, it is better to use a classification algorithm. \\

With classification you need to decide upon the labels (classes) you want to use beforehand, and then for each class find a small subset as training data. Afterwards the classifier can automatically label a new document to one of the labels. Before we can enter the training data or classify a text document we need to pre-process the document into valid input and define which classes are important.

\subsubsection{defining classes}
Since we decided on using classification as our supervised machine learning algorithm we need to define the classes beforehand. Since we are not familiar with the most useful categories for measuring relationships between cities, we asked our clients Evert Meijers and Antoine Peris for their help. They choose the following topics for us to investigate:\\
- Commuting ('woon-werk') \\
- Shopping or servicetrips ('winkelen / diensten') \\
- Leisure trips ('vrije tijd / recreatie / toerisme') \\
- Residential mobility ('verhuizingen') \\
- Business trips ('zakelijk verkeer') \\
- Education ('Onderwijs, School') \\
- Collaboration ('Samenwerken') \\
- Goods Transportation ('Goederenvervoer') \\
- Other ('overig / restcategorie') \\
\\


\subsubsection{preprocess}

For text preprocessing there are a number of tools available. We used NLTK \cite{nlkt_stemming} for tokenizing, removing stopwords and stemming and Gensim \cite{Gensim} for Modelling.
\begin{enumerate}
\item Tokenization. \\ splitting up the text into words and other symbols called tokens. For this we implemented the tokenizer NLTK provides. Testing showed that it does what it is supposed to do.
\item Removing stop words. \\ Removing all common words (the, a, an etc) and symbols ('.', ',', '!', etc). For removing stopwords we used a list from NLTK containing dutch stopwords. For removing symbols we made a method that removes all non alphabet values except spaces from text. 
\item Stemming. \\ Reducing derived words to their stem (e.g. fishing -> fish). We first wanted to use NLTK, which itself implements several stemmers including SnowBallStemmer \cite{snowball_dutch} and Porter \cite{porter_Stemmer}. However research from the Rijks Universiteit Groningen (RUG) by Tanja Gaustad and Gosse Bouma \cite{gaustad2002accurate} evaluated a Bayesian text classification system with either no stemming or the Porter or dictionary based stemmer. Which concludeded stemming does not lead to significant change in classification accuracy.
\item Modelling (TF-IDF). \\ Before we can enter this tokenized data into the machine learning algorithm we first need to transform it to the correct model. Gensim provides two ready to use method for this to change tokenized text into the proper input data. The first is token2ID which gives each unique word a numeric ID and counts the amount that those words occur in a text and the second is tfidf which changes the amount that words occur in tf-idf values. tf-idf means term frequence inverse documet frequency. Term frequency is the number of times the word occurs in a document and idf is the number of documents devided by the number of documents where the term occurs. These two values are then multiplied. The results are normalized to make up for different document lenghts [https://radimrehurek.com/gensim/models/tfidfmodel.html].
\end{enumerate}

\subsubsection{training}
In order to use supervised machine learning, we first need to find training data for each class. We thought about two ways to do this. In order to get a good training sample for these categories, we discussed two different methods. \\

The first option was to query for results from news(paper) sites. However, this approach did not give us the desired results. The reason for this is that the categories we find in the newspapers do not match the categories Antoine and Evert provided us with. We also tried using the search engine from those news sites, however using those we found completely unrelated articles to the search queries (A search query about 'verhuizen' resulted in an article about parrots and minecraft).\\

The second option we are considering is to use Google Custom Search to obtain results from Google using the categories/keywords Antoine and Evert provided us with. A quick test with this last method provided us with quite good results (although we still see some noise in the results this is less than with the search engines from news sites).\\


\subsubsection{classifying}
For classifying there are a multitude of algorithms available. For choosing the classifier we make use of the Microsoft Azure Machine Learning Test Sheet \cite{MLCheatSheet}. Several factors should be taken into account when choosing an algorithm. These are:
    \begin{enumerate}
        \item Accuracy - How well the algorithm separates the websites.
        \item Training Time - How long it takes to train the algorithm.
        \item Linearity - Linear regression assumes data trends follow a straight line. This is trade-off between accuracy and speed.
        \item Number of Parameters - Adjustable parameters increase the flexibility of the algorithms. This is a trade-off between training time and accuracy.
        \item Number of Features - A large number of features can make some algorithms unfeasibly long. Especially text data (what we are using!) has a large number features. Support Vector Machines are especially well suited in this case.
        \item Special Cases - Some learning algorithms make particular assumptions about the data or the results.
    \end{enumerate}
    
    For textual data especially support vector machines are recommended, so it is most likely we will choose that machine learning algorithm. Depending on whether we have time we might do some tests before making our decision however. The sci-kit package\cite{scikit-learn} provides an easy to implement module to use this.


\begin{comment}
First, one or more subjects should be extracted from each relevant document. These subjects should reflect the contents of the document well in order to adequately identify relationships.
The second sub-task is to determine the relationships between the cities mentioned in the document, based on the extracted subjects.

% \\
% The first challenge we face is to extract one or more subjects from each document that we selected in the previous step. These subjects should be a good description of what is stated in the document and will be the main source of identifying the relations between the cities cited in the document.
% \\
% The second challenge is, using the former mentioned subjects, to identify relations based on these subjects. 
% \\
% We will consider several possibilities which can help us in tackling these challenges in the sections below.


\subsubsection{Extracting Subjects from Documents}
To identify the subjects associated with each individual document, we investigated several algorithms that we will present next.

\begin{description}
    \item[RAKE] offers an unsupervised, domain-independent, and language-independent method for extracting keywords from individual documents \cite{rose2010automatic}. This fits well within the context of our application. We work with individual documents which we want to label with one or more subjects/keywords.
    \item[Gensim]
    \item[NLTK]
\end{description}


\subsubsection{Identifying Relations Based on Subjects}


\end{comment}