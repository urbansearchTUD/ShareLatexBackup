\section{Gathering the Data}\label{sec:4-gathering}
As explained in section \ref{sec:design-goals}, data sources should be plugable. An initial corpus of documents is needed for the project, which we will choose in this section. Nowadays many people have access to the Web, and for a lot of people the Web is probably also their primary source of information. Next to that, the Web contains vast amounts of documents which could shed some light on relations between cities. Therefore, the decision was made to use web-data as a data source. To avoid duplicate work, which could occur by randomly searching the web, an obvious choice is to use Common Crawl as a data source.

\subsection{Common Crawl} \label{sec:commoncrawl}
Common Crawl \cite{commoncrawl} is a freely accessible corpus of pages across the web, updated and released on a monthly basis. Many researchers have used the data for various purposes~\cite{smith2013dirt, muhleisen2012web, singh2012wikilinks}. Since the project requires analysis on a very large set of documents, the corpus is a very suitable candidate for us to work with. \\

The data from Common Crawl comes in three    formats\footnote{\url{https://gist.github.com/Smerity/e750f0ef0ab9aa366558}}: 
\begin{description}
\item[\textbf{WARC}] This is the default and most verbose format. It stores the HTTP-response, information about the request and meta-data on the crawl process itself. The content is stored as HTML-content.
\item[\textbf{WAT}] Files of this type contain meta-data, such as link addresses, about the WARC-records. This meta-data is computed for each of the three types of records (meta-data, request, and response). The textual content of the page is not present in this format.
\item[\textbf{WET}] This format only contains extracted plain text. No HTML-tags are present in this text. For our purposes, this is the most useful format.
\end{description}

Common Crawl stores these pages in the following way: each archive is split into many segments, with each segment representing a directory. Every directory contains a document listing file and a folder for each file format (WARC, WAT and WET), which in turn contains the compressed pages belonging to the segment. To be able to efficiently get a single page, Common Crawl indexes the segments to directly map URLs to document locations using an offset and length which can be found using the Common Crawl index\footnote{\url{http://index.commoncrawl.org}}. A single index is a combination of multiple key-value pairs, for an example of a single index see listing \ref{lst:cc-index}. An index contains important information such as for example the name of a WARC file, and the index and offset to find the correct data within that WARC file. Since WAT- and WET-files can be generated from WARC-files, they only provide such indices for WARC-files. If no file index is provided with a data request, an aggregated compressed file of all files of the requested format is returned.

\begin{lstlisting}[language=Python, caption=Common Crawl index example, label={lst:cc-index}, numbers=none]
{"urlkey": "nl,tudelft)/", "timestamp": "20170323161043", "status": "200", "url": "http://www.tudelft.nl/", "filename": "crawl-data/CC-MAIN-2017-13/segments/1490218187144.60/warc/CC-MAIN-20170322212947-00594-ip-10-233-31-227.ec2.internal.warc.gz", "length": "6837", "mime": "text/html", "offset": "727926652", "digest": "WPTH3FM5VR7UGLA5PZS5L5YI22TNIKXG"}
\end{lstlisting}

For extracting data from Common Crawl, many open-source libraries are available. Common Crawl's official website refers to \texttt{cdx-index-client}\footnote{\url{https://github.com/ikreymer/cdx-index-client}} as a command line interface to their data indices. It allows for, among others, specifying which data set to use, supports multiple output formats (plain text, gzip or JSON) and can run in parallel. Since this library only retrieves the file indices, we need another way to actually retrieve the pages pointed to. However, there is a problem with this: we are only interested in WET-files, but Common Crawl does not have WET-files indexed. We would therefore have to collect the WARC-files and convert them to WET-files ourselves, requiring us to parse HTML for every document we are interested in.

As mentioned in the design goals section not all available web-data will be used due to limited resources. A simple query \texttt{url=*.nl\&output=json\&showNumPages=true} on the CC-MAIN-2017-13 index using the online interface\footnote{\url{http://index.commoncrawl.org/CC-MAIN-2017-13-index?url=*.nl\&output=json\&showNumPages=true}} yields 1676 pages. Pages in this sense are listings of 15000 indices, so there are roughly 25 million entries in total out of the 2.94 billion pages available in Common Crawl. It is very important to note that searching for a top level domain like \texttt{.nl} only includes the first page of every matching domain. To get all pages, additional queries for each site with more than one page are to be performed.


\subsection{Other Data Sources} \label{sec:delpher}
Besides Common Crawl, there are a plethora of other sources that might contain valuable information. The most notable is the Dutch royal library, Delpher\footnote{\url{http://delpher.nl}}. It contains millions of Dutch digitalised newspapers, books and magazines from the fifteenth century up until about 1995. Because of this, it is a useful resource for historical research. Additionally, Statistics Netherlands\footnote{\url{https://www.cbs.nl/en-gb}} is the governmental organisation collecting statistical data about the Netherlands and comes with an API, making most of their data publicly accessible. The NOW Corpus\footnote{\url{http://corpus.byu.edu/now/}} collects newspaper and magazine articles through Google News and provides several tools to perform queries on this data. It can also be downloaded. 

Due to time and resource constraints, we have chosen to exclude these from the project. Of course, in future versions, other data sources could be included.