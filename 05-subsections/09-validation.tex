\section{Evaluation and Verification} \label{eval_veri_results}

To ensure code quality in our project we used several methods. The results from SIG \cite{sig}, a tool to ensure code quality and maintainability, are discussed and the testing is discussed.

\subsection{Testing}

\subsubsection{Unit tests}
\todo{coverage}

\subsubsection{Integration Tests}

\subsubsection{System Tests}

\subsubsection{Acceptance tests}



\subsection{SIG \& BetterCodeHub}
SIG, Software Improvement Group, gives detailed insight needed to achieve better code quality and maintainability. SIG rates the code on a five star scale based on nine different values concerning code quality. Before submitting code to SIG we used BetterCodeHub\cite{better_code_hub} to check for possible faults in our code. BetterCodeHub does partly what SIG also does, but it is done online instead and can be done on every moment. Code was submitted to SIG on week 5 and week 9 of the project. Since the final report is due to the same date as the second submission for SIG review, the second review will not be included in this report. Instead we will show the final results from BetterCodeHub for week 9. Exact feedback can be found in appendix \ref{sig_fb}.

\subsubsection{week 5}
The first feedback from SIG was in the fifth week of development. Before uploading on BetterCodeHub our code passed all checks. For SIG it had a score from four out of five stars which means our code is above average maintainable. The last star was missed because the code is above average complex. This means that some of the functionality of some methods should be split into separate methods.
\todo{fixed this?}

\textbf{week 9} \\



\subsection{Evaluating the Classification}
\subsubsection{Accuracy}

\subsubsection{Confusion Matrix}

\subsubsection{Precision, Recall, F1 and UAC}


\subsection{Evaluation of Relation Scores}


