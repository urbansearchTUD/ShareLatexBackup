\section{Validation}
In this section, we first define a protocol with which the results of the system can be evaluated for correctness. Then, we describe how the system is tested and how we measure the quality of the code.

\subsection{Testing the Application}
We will test the program using four different testing methods. The first is unit testing, which tests the separate components individually. Integration testing for testing how well different components work together. system testing for testing the different entire system and Acceptance testing for testing how well the clients think the program works.

\subsubsection{Unit Tests}
Unit testing is done by writing automatic tests and making sure they pass every time the tests are executed. Unit tests test each method of a function separately, checking that the method does what it is supposed to do. If the method would need information from outside the class that information is mocked. This means that instead of using that other class, a fake object is made which returns a fake value. This ensures the tests will never fail due to changes in other classes.

\subsubsection{Integration Tests}
Integration testing uses automated tests which test how well different components of the system work together. This is done more or less the same as unit testing, however whilst you would mock methods from other classes in unit testing, with integration testing you do not. It is assumed that the separate modules are unit tested, therefor if an error occurs it is because something is wrong with the interaction between the modules and not with the modules themselves. 

\subsubsection{System Tests}
We are also planning to use system testing. System testing provides a more complete test of the entire system. This means it is useful to detect faults in the overall system, but less easy to determine where these faults may be located. System testing is done manually, which means the tests can not be easily repeated when the system changes whilst with other testing techniques this is possible.

\subsection{Validating the Application's Results}
For validating the results of the application, we use acceptance testing. This is testing done to see if the software does what the clients are expecting it to do. These tests are therefore also executed by the clients manually. Afterwards they can say what worked, what did not work, what was missing and what could be improved. For this, we set up an evaluation protocol.\\

\subsubsection{Evaluation Protocol}\label{sec:validation_protocol}
To be able to verify the relations that the system identifies, we need a protocol. Two parts have to be validated: (1) classification of documents and (2) relation scores. These are related in the sense that a relation score is calculated by the number of occurrences in labelled documents, so the correctness of labelling affects the correctness of relation scores.\\

Classification of documents is to be evaluated as follows. The client, considered a professional, labels a predefined set of documents, $D$ of size $d$. The result set is called $C$. This same document set $D$ is fed to the classification algorithm, which results in set $A$. The result sets $C$ and $A$ are then compared. The accuracy $r$ of the classification algorithm is determined by the following formula: $r = \frac{|C \cap A|}{d}$. For example, if $d=40$ and $|C \cap A|=20$, $r$ would be $0.50$. The classification algorithm is then determined to have an accuracy of 50\%.\\

Evaluating relation scores is done differently. An important factor here is that cities have a natural relation due to their geographical position \cite{tobler1970computer}, so one would expect cities that lie close to each other are more related than cities that are on different sides of the country. This natural relation can be represented using the Gravity Model by Reilly \cite{reilly1931law}. The Gravity Model describes that the expected relation between two cities is based on the population of the two cities and the distance between these cities. A relation between two cities that is extracted from the data should thus expose a similar relative score as they would for the gravity model. Consider for example Amsterdam and Hoofddorp, which are cities that lie close to each other. Amsterdam is a large city, whereas Hoofddorp is much smaller. However, due to their close geographical position, the score that results from the Gravity Model would be high. If they turn out to have a very high score in our system, that would imply that the system is correct. Besides the Gravity Model, one can rely on the intuition of a professional in the field of urbanism that can judge whether an extracted relation is close to reality or not. We therefore agreed with the client that they would decide on a small set of relations whether they are 

\subsection{SIG}
SIG \cite{sig}, short for software improvement development group, is an organisation that analyses the code of projects to give insights in the quality of how the code is written. A high score means the code is highly maintainable and is kept simple. SIG includes Better Code Hub \cite{better_code_hub} which checks our code according to 10 guidelines as can be seen in appendix \ref{bch_guidelines}. The great thing about Better Code Hub is that it can be run at anytime. We can check Better Code Hub whenever, whilst for SIG we have to send in our code and wait for feedback.