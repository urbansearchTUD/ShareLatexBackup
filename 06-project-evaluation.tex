\chapter{Project Evaluation}
\todo{intro}
% evaluate the product, and see whether we have at least the minimum viable product
% using the requirements and design goals (possibly in a table)
% make sure to add lots and lots of figures, tables and benchmarks here
% also add SIG stuff and talk about SCRUM

\todo{Put results in appendix F}
\todo{write conclusion about results}

\section{Fulfilment of Requirements}
In section \ref{sec:reqs} we declared the requirements for our program. Table \ref{requirements_pass/fail} shows which of these requirements passed or failed. \\
\todo{failed must have}
The program works as intended so all must haves passed. \\
There are however two should haves which failed. Finding correct relations proved more time-consuming than expected therefor our algorithm only discerns the top level relations (e.g. trade) and not sub levels (e.g. food trade). Furthermore there are a lot of places with duplicate names, yet no complete lists of these duplicates are available. Therefor it is less easy implementable than first thought. \\
Since other, more important, tasks took longer to implement than intended we did not make implement functionality to use Delpher to characterise relationships. We did add functionality to visualise the data by using a map. \\
As expected the would likes did not pass. It is theoretically possible to show all connections of all places on the map at the same time. However, it would result in a completely filled in map because there is a line for each relation so one would not be able to get any useful information from this.

\todo{more about not mentioned pass/fail, name reqs?}
\todo{Kan die tabel niet beter Requirement:Pass/Fail:Uitleg, en dan voor Must, should etc? ja}
\begin{table}[h]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{lll}
Must Haves                     & Pass / Fail & Comment \\
1 Mining from commoncrawl      & Pass        &         \\
2 Exporting relations          & Fail        &         \\
3 Extracting relations         & Pass        &         \\
4 Visualisation                & Pass        &         \\
5 Present statistics           & Pass        &         \\
                               &             &         \\
Should Haves                   & Pass / Fail & Comment \\
1 Hierarchical relations       & Fail        &         \\
2 Machine learning retrainable & Pass        &         \\
3 Add large datasets           & Pass        &         \\
4 Duplicate city names         & Fail        &         \\
                               &             &         \\
Could haves                    & Pass / Fail & Comment \\
1 Use Delpher                  & Fail        &         \\
2 Visualisation for comparing  & Pass        &         \\
                               &             &         \\
Would likes                    & Pass / Fail & Comment \\
1 Show all connections         & Pass        &         \\
2 Other than .nl data          & Fail        &        
\end{tabular}
\end{table}

\section{Answering the research question}


As mentioned in section \ref{sec:problem-definition-analysis}, the main problem was the following:
\begin{quote} 
\centering 
\textit{How can open data be leveraged such that a metric for the strength of relationships between cities can be defined and visualised?}
\end{quote}

To answer this question we came up with several sub-problems. These sub-problems, together with their answer and the reason for their importance are the following:
\begin{itemize}
    \item How can we filter the available text data to find co-occurrences of cities and discarding text data that does not contain co-occurrences? \\
    
    As shown in section \ref{sec:5-filtering} the python package pyahocorasick can be used to check the text data and try to match words within the text data. However, the algorithm also checks words that are part of compound words. Therefor, in case that is not wanted, the package will have to be extended to add this extra control. \\
    
    This should reduce the amount of data and thereby potentially speed up the rest of process.
    
    \item The sub-problem that arises after filtering is how to determine what relationships can be extracted from the text-data, this will be referred to as the classification of the text-data. \\
    
    In section \ref{5-classification} ... \todo{}
    
    This requires a method that reliably and efficiently processes the text-data and can be tuned to the clients wishes, meaning that the classification should output what the client desires. 
    
    \item The next question is how to store the data and determine the strength of the relationships. \\
    
    Section \ref{sec: 5-storing} shows that we need storage for the text documents, as well as for extracted relations. The text documents can simply be stored on the disk. For the extracted relations the graph database Neo4j can be used. \\
    For determining the strength of relationships the classifier is used. We simply count the numbers of documents that have been found for each relation and use those numbers as the strength of the relationships. \\
    
    Storage of documents is important for when the classifier is improved upon so that not everything will have to be downloaded again before it can be classified again. Determining the strength of the relations is important for the research done based on this.
    
    \item The last question is how to combine the stored data and present it to a user, this means visualising and/or exporting the data in an accessible way.\\
    
    As for visualising the data, section \ref{sec: 5-frontend} shows 
    \\
    \todo{}
    b
\end{itemize}

From these sub-results we can answer the main research question. One way open data can be leveraged such that a metric for the strength of relationships between cities can be defined and visualise by first downloading text data from a storage (in our case documents from CommonCrawl). Each document is checked for containing two or more city names by using the pyahocorasick package and is discarded if it does not meet the check. This selection of documents is then classified according to pre-defined relationships between cities using the svm machine learning algorithm. The documents are stored on the disk and the relations are stored in the visual graph database Neo4J. The strength of each relationships between two cities is then found by counting the number of all documents for each relationship that contain the two city names. This is visualised by NodeJS \todo{?}

\section{Process}

\subsection{Collaboration Between the Team Members}
The collaboration between the team members went well. The team members worked in a room in the faculty of architecture from 9-5 each day. Three of the four team members knew each other already. The work was divided even over the team members. 

\subsection{Collaboration Between the Team Members and the TU Delft Coach}
Each week 9:30 on Monday the team members had a meeting with the TU Delft coach. In the beginning there were some communication issues between the team and the coach but as the process went on communication became better.
\todo{Claudia absent twice}

\subsection{Collaboration Between the Team Members and the Client}
The collaboration between the team members and the client was good as well. 
Weekly meetings helped the team members making the product as good as possible to the clients wishes. 