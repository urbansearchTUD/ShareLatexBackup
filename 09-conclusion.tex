\chapter{Conclusion}

% \todo{ Start with "In the past few months we blah" and shortly mention the chapters
%  Mention the project goal and how well it is met, without duplicating the 
%  evaluation chapter}

% In this report, we explored the problem domain to be able to define and analyse the problem. Doing so, the product requirements were extracted in order to prepare for an attempt to solve the problem.

% First, we discussed related work. We saw that there are many methods that try to estimate the flows between cities. However, all of these approaches turned out to be either very limited or questionable. Some other approaches that use digital content for estimating these relations looked promising but are small scale. Web data contains many relations that were overlooked by methods. An example is the method where researchers looked at the different locations where businesses are located. \\

% We saw that there are currently two methods for analysing the relations between cities. Manually analysing search engine data is very slow and requires a lot of man-hours and looking at the different locations where businesses are located is only interesting for the economic relation and still misses a lot of data.
% \\

% Second, we identified the requirements for a solution to the problem and discuss issues that might arise. The used the MoSCoW model to describe the importance of the different requirements. The most important must haves we found are being able to input place names, displaying a map with the connection data and being able to extract this data.
% \\

% Third, we described a framework that satisfies the requirements and tackles the issues. We decided to start by using data from Common Crawl, although we might later extend this to other data sources such as Delpher. After selecting relevant data (data which contains 2 or more city names) we store the data with Neo4j. We then use a classifier to group the data. We use this on all data to extract categories and process these per pair of cities to see what the important connection types for each city are. To visualise this data we use the graph Neo4j provides.
% \\

% Finally, we discussed how to verify and validate the (intermediary) results from the system. The code is tested using four common types of testing: unit testing, integration testing, system testing and acceptance testing. All types serve a different purpose and together should indicate how well the system performs. Additionally, the code is sent to the Software Improvement Group for quality analysis.
% \\

% With this setup we should be able to make a well tested, functioning system that meets the requirements of our clients. Furthermore, using this system will enable us to answer the question "how can the strength of relationships between cities be extracted and visualised from open data?" 

% \todo{\todo{new stuff}}




% \section{Answering the research question}


% As mentioned in section \ref{sec:problem-definition-analysis}, the main problem was the following:
% \begin{quote} 
% \centering 
% \textit{How can open data be leveraged such that a metric for the strength of relationships between cities can be defined and visualised?}
% \end{quote}

% To answer this question we came up with several sub-problems. These sub-problems, together with their answer and the reason for their importance are the following:
% \begin{itemize}
%     \item How can we filter the available text data to find co-occurrences of cities and discarding text data that does not contain co-occurrences? \\
    
%     As shown in section \ref{sec:5-filtering} the python package pyahocorasick can be used to check the text data and try to match words within the text data. However, the algorithm also checks words that are part of compound words. Therefor, in case that is not wanted, the package will have to be extended to add this extra control. \\
    
%     This should reduce the amount of data and thereby potentially speed up the rest of process.
    
%     \item The sub-problem that arises after filtering is how to determine what relationships can be extracted from the text-data, this will be referred to as the classification of the text-data. \\
    
%     In section \ref{5-classification} ... \todo{}
    
%     This requires a method that reliably and efficiently processes the text-data and can be tuned to the clients wishes, meaning that the classification should output what the client desires. 
    
%     \item The next question is how to store the data and determine the strength of the relationships. \\
    
%     Section \ref{sec: 5-storing} shows that we need storage for the text documents, as well as for extracted relations. The text documents can simply be stored on the disk. For the extracted relations the graph database Neo4j can be used. \\
%     For determining the strength of relationships the classifier is used. We simply count the numbers of documents that have been found for each relation and use those numbers as the strength of the relationships. \\
    
%     Storage of documents is important for when the classifier is improved upon so that not everything will have to be downloaded again before it can be classified again. Determining the strength of the relations is important for the research done based on this.
    
%     \item The last question is how to combine the stored data and present it to a user, this means visualising and/or exporting the data in an accessible way.\\
    
%     As for visualising the data, section \ref{sec: 5-frontend} shows 
%     \\
%     \todo{}
%     b
% \end{itemize}

% From these sub-results we can answer the main research question. One way open data can be leveraged such that a metric for the strength of relationships between cities can be defined and visualise by first downloading text data from a storage (in our case documents from CommonCrawl). Each document is checked for containing two or more city names by using the pyahocorasick package and is discarded if it does not meet the check. This selection of documents is then classified according to pre-defined relationships between cities using the svm machine learning algorithm. The documents are stored on the disk and the relations are stored in the visual graph database Neo4J. The strength of each relationships between two cities is then found by counting the number of all documents for each relationship that contain the two city names. This is visualised by NodeJS \todo{?}
